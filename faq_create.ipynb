{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741d1dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean FAQ dataset saved: 1016105 pairs\n",
      "                                                 question  \\\n",
      "696839  The only number that appears is a fax in Chile...   \n",
      "639630  I need some immediate help, please, with docum...   \n",
      "95263   BA0455 man on crutches in exit aisle, 9F. Safe...   \n",
      "69234   yo website ain‚Äôt working can you please take a...   \n",
      "556414      when we gonna fix this IÔ∏è IÔ∏è IÔ∏è issue? ü§∑üèΩ‚Äç‚ôÄÔ∏èüôÑ   \n",
      "\n",
      "                                                   answer  \n",
      "696839  This is the telephone number for enquires that...  \n",
      "639630  Hey Jaime, to further investigate please write...  \n",
      "95263   Hi Paul, did you discuss this with the crew on...  \n",
      "69234   We're sorry that you're having an issue! Pleas...  \n",
      "556414  Here‚Äôs what you can do to work around the issu...  \n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This script reads a CSV of Twitter customer service interactions,\n",
    "             filters relevant columns, separates customer and agent messages,\n",
    "             merges them into question-answer pairs, cleans the text, and saves\n",
    "             the result as a CSV for FAQ purposes.\n",
    "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df = pd.read_csv(\"twcs.csv\", low_memory=False)\n",
    "\n",
    "# Keep relevant columns\n",
    "df = df[[\"tweet_id\", \"author_id\", \"inbound\", \"response_tweet_id\", \"text\"]]\n",
    "\n",
    "# Convert IDs to string for merging\n",
    "df[\"tweet_id\"] = df[\"tweet_id\"].astype(str)\n",
    "df[\"response_tweet_id\"] = df[\"response_tweet_id\"].astype(str)\n",
    "\n",
    "# Separate customer and agent messages\n",
    "customer_msgs = df[df[\"inbound\"] == True]\n",
    "agent_msgs = df[df[\"inbound\"] == False]\n",
    "\n",
    "# Merge customer queries with corresponding agent responses\n",
    "faq_pairs = customer_msgs.merge(\n",
    "    agent_msgs,\n",
    "    how=\"inner\",\n",
    "    left_on=\"response_tweet_id\",\n",
    "    right_on=\"tweet_id\",\n",
    "    suffixes=(\"_customer\", \"_agent\")\n",
    ")\n",
    "\n",
    "# Keep only text columns\n",
    "faq_pairs = faq_pairs[[\"text_customer\", \"text_agent\"]].dropna()\n",
    "\n",
    "# ---- CLEAN TEXT ----\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)         # remove mentions\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)      # remove URLs\n",
    "    text = re.sub(r\"\\s+\", \" \", text)         # collapse multiple spaces\n",
    "    return text.strip()\n",
    "\n",
    "#Apply clean_text to customer and agent messages\n",
    "faq_pairs[\"question\"] = faq_pairs[\"text_customer\"].apply(clean_text)\n",
    "faq_pairs[\"answer\"] = faq_pairs[\"text_agent\"].apply(clean_text)\n",
    "\n",
    "#keep only cleaned question-answer pairs\n",
    "faq_pairs = faq_pairs[[\"question\", \"answer\"]]\n",
    "\n",
    "#save the cleaned FAQ dataset \n",
    "faq_pairs.to_csv(\"faq_auto.csv\", index=False)\n",
    "print(\"Clean FAQ dataset saved:\", len(faq_pairs), \"pairs\")\n",
    "print(faq_pairs.sample(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16097c98",
   "metadata": {},
   "source": [
    "# Download sentence-transformers library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d378a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (5.1.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (4.46.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (4.67.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (1.1.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (1.10.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (0.26.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (10.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc61d9f2",
   "metadata": {},
   "source": [
    "# AUTO FAQ Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4a7bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "#Load pre-trained Sentence-BERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "answers = faq_pairs[\"answer\"].tolist()\n",
    "embeddings = []\n",
    "\n",
    "batch_size = 128  # Batch processing to avoid memory issues\n",
    "for i in range(0, len(answers), batch_size): \n",
    "    batch = answers[i:i+batch_size]\n",
    "    emb = model.encode(batch, convert_to_numpy=True)\n",
    "    embeddings.append(emb)\n",
    "\n",
    "embeddings = np.vstack(embeddings) #combine all batches into a single Numpy array\n",
    "np.save(\"answer_embeddings.npy\", embeddings) #save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a02cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
